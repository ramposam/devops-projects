services:
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data # Add persistent volume for Redis

  airflow:
    image: apache/airflow:latest
    build:
      context: .
    container_name: airflow
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: "CeleryExecutor" # Use CeleryExecutor for distributed environments.
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres:5432/airflow"
      AIRFLOW__CORE__FERNET_KEY: "9SDPnoaCrci1aLyM3GyMP7nYOnIc7YVS3ll-a1wPlMw=" # Generate a random Fernet key using the cryptography library.
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__CORE__REMOTE_LOGGING: "True"
      AIRFLOW__CORE__REMOTE_LOG_CONN_ID: "S3_CONN_ID" # Create this connection in the Airflow UI after startup.
      AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER: "s3://bucket-name/logs/"
      AIRFLOW__CORE__BASE_LOG_FOLDER: "/opt/airflow/empty_logs"
      AIRFLOW__CELERY__BROKER_URL: "redis://redis:6379/0"
      AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://airflow:airflow@postgres:5432/airflow"
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "60"  # Refresh interval set to 60 seconds
      AIRFLOW__SCHEDULER__DAG_DIR_SCAN_INTERVAL: 30

    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "airflow db init &&
      airflow users create --username admin --password admin --firstname Airflow --lastname Admin --role Admin --email admin@example.com &&
      airflow webserver & airflow scheduler & airflow celery worker"

    restart: always

volumes:
  postgres_data:
  redis_data: # Declare persistent volume for Redis
